# July 6th Lecture
## Joint Probability Distributions
- In most engineering problems, it is necessary to consider more than one random variable, which thus requires additional information that quantifies the probabilistic relationship between the random variables - **joint distributions**
- The **joint probability density function** is defined as:
    - $f_{X, Y}(x, y)dxdy=P(x < X \leq x + dx \cap y < Y \leq y + dy)$
    - The probability of $X$, $Y$ being in the square region $(a < X \leq b \cap c < Y \leq d)$ is:
        - $P(a < X \leq b \cap c < Y \leq d) = \int_a^b \int_c^d f_{X,Y}(u, v)dvdu$
    - Joint probability density functions still hold the properties of single-variable probability density functions:
        - $f_{X, Y} \geq 0$
        - $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X, Y}(x, y)dxdy = 1$
- The **joint cumulative distribution function** is defined as:
    - $F_{X, Y} = P(X \leq x \cap Y \leq y) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X, Y}(u, v)dudv$
    - The joint probability density function can be obtained from the joint cumulative distribution function by taking the second order mixed partial derivative:
        - $f_{X, Y}(x, y) = \frac{\partial^2}{\partial x \partial y}F_{X, Y}(x, y)$
## Marginal Probability Distributions
- Given the joint distribution of $X$ and $Y$, one can easily obtain the distribution of $X$ or $Y$ alone - known as the **marginal distribution**
    - Marginal Distribution of X: $f_x(x) = \int_{-\infty}^{\infty}f_{X, Y}(x, y)dy$
    - Marginal Distribution of Y: $f_y(y) = \int_{-\infty}^{\infty}f_{X, Y}(x, y)dx$
- Evaluating the joint cumulative distribution function at $x = \infty$ (or $y = \infty$) yields the marginal cumulative distribution function
    - $F_X(x) = F_{X, Y}(x, \infty)$
    - $F_X(y) = F_{X, Y}(\infty, y)$
## Conditional Probability Distributions
- For continuous random variables, the conditional distribution of $X$ given $Y$ is
    - $f_{X | Y}(x|y)dx = P(x \leq X \leq x + dx | y < Y \leq y + dy)$
    - Using the definition of conditional probability, this can be found as: 
        - $f_{X | Y}(x|y)dx = \frac{P(x < X \leq x + dx \cap y < Y \leq y + dy)}{P(y < Y \leq y + dy)}$
        - This is equivalent to $f_{X |Y}(x|y)=\frac{f_{X, Y}(x, y)}{f_Y(y)}$
- To find the conditional cumulative probability distribution, the conditional probability density function can be integrated
    - $F_{X|Y}(x | y) = \int_{-\infty}^{x}f_{X|Y}(u|y)du$
## Independence
- $X$ and $Y$ are said to be independent random variables if for all $y$ it is the case that $f_{X|Y}(x|y) = f_X(x)$ or equivalently:
    - $f_{Y|X}(y|x) = f_Y(y)$
    - $f_{X, Y}(x, y) = f_X(x)f_Y(y)$
    - $F_{X|Y}(x|y) = F_X(x)$
    - $F_{Y|X}(y|x) = F_Y(y)$
    - $F_{X, Y}(x, y) = F_X(x)F_Y(y)$
## Jointly Distributed Discrete Random Variables
- The rules for the aforementioned distributions also apply to discrete random variables
    - Joint PMF: $p_{X, Y}(x, y) = P(X = x \cap Y = y)$
    - Conditional PMF: $p_{X | Y}(x | y) = \frac{p_{X, Y}(x, y)}{p_Y(y)}$
    - Marginal PMF: $p_X(x) = \sum_{all \; y_i}p_{X, Y}(x, y_i)$ 
